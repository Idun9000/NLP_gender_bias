{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "829f94ba-e5a8-463f-87d7-d5c36c170e56",
   "metadata": {},
   "source": [
    "## Activate virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9319044b-edd3-4423-ac5b-db8ea752e3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec virt_env in /home/ucloud/.local/share/jupyter/kernels/virt_env\n",
      "Done! Remember changing the kernel in Jupyter.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path='/work/NLP_exam'\n",
    "os.chdir(path)\n",
    "\n",
    "# Make the activate.sh file executable\n",
    "!chmod +x activate.sh\n",
    "\n",
    "# Now run the script\n",
    "!./activate.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684c76c6-4598-477c-ac97-30fb1dca893a",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538dee55-1157-4f53-98ab-fa4fc1744839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde98ba4-00d4-4c0e-8c00-792c25c7098d",
   "metadata": {},
   "source": [
    "## Load in json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9886353-3311-42ad-93f2-fac8183579d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your JSON file\n",
    "file_path = '/work/NLP_exam/tokenized_data_by_year.json'\n",
    "\n",
    "# Load the JSON file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d37dc-9c36-4c92-995d-ff1aa25ee0e7",
   "metadata": {},
   "source": [
    "## Checking that the dictionary structure works as it should"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0fa84a1-0b7d-4d70-8f15-1450b1b16a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years available in the dataset:\n",
      "['1973', '1992', '1936', '1968', '1995', '1956', '1997', '1987', '1964', '1996']\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "# Print the keys (years)\n",
    "print(\"Years available in the dataset:\")\n",
    "print(list(data.keys())[:10])\n",
    "\n",
    "# Get all the years (keys)\n",
    "years = list(data.keys())\n",
    "print(len(years)) # check that all the data is there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9270052a-1573-4ce3-b6ff-16face25d877",
   "metadata": {},
   "source": [
    "## Look at the amount of books for different years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af3a77f-6d43-41c9-bb32-9de8e69857a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n",
      "313\n",
      "344\n",
      "33\n",
      "25\n",
      "41\n",
      "38\n",
      "26\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "print(len(data['1998']))  # Number of tokenized books in 1998\n",
    "print(len(data['1999']))  # Number of tokenized books in 1999\n",
    "print(len(data['2000']))  # Number of tokenized books in 2000\n",
    "\n",
    "print(len(data['1900']))  # Number of tokenized books in 1900\n",
    "print(len(data['1901']))  # Number of tokenized books in 1901\n",
    "print(len(data['1902']))  # Number of tokenized books in 1902\n",
    "print(len(data['1903']))  # Number of tokenized books in 1903\n",
    "print(len(data['1904']))  # Number of tokenized books in 1904\n",
    "print(len(data['1905']))  # Number of tokenized books in 1905"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95bf4e9-1d87-4bab-bb62-e1e2138b844d",
   "metadata": {},
   "source": [
    "## Make a function that trains and saves word embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd7f17-1aec-4d7e-9b5a-f20e70e50516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of books for 1900-1905: 198\n"
     ]
    }
   ],
   "source": [
    "# Define output directory\n",
    "output_dir = \"/work/NLP_exam/models_1900_2000\"\n",
    "\n",
    "# Define time periods (for each year, 5-year window from that year)\n",
    "time_periods = [(year, year + 5) for year in range(1900, 2000)]\n",
    "\n",
    "# Word2Vec parameters\n",
    "num_features = 300  # Word vector dimensionality\n",
    "min_word_count = 5  # Minimum word count\n",
    "context = 10  # Context window size\n",
    "downsampling = 10e-5  # Downsample setting for frequent words\n",
    "\n",
    "# Function to train and save models\n",
    "def train_and_save_models(data, time_periods, output_dir, **word2vec_params):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for start_year, end_year in time_periods:\n",
    "        # Combine all tokenized texts for the time period\n",
    "        combined_texts = []\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            if str(year) in data:  # Ensure the year exists in the data\n",
    "                combined_texts.extend(data[str(year)])  # Add all books from the year\n",
    "        \n",
    "        if not combined_texts:  # Skip if no data is found for the time period\n",
    "            print(f\"No data available for the period {start_year}-{end_year}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Number of books for {start_year}-{end_year}: {len(combined_texts)}\")\n",
    "\n",
    "        # Train the Word2Vec model\n",
    "        model = Word2Vec(sentences=combined_texts, **word2vec_params)\n",
    "        \n",
    "        # Save the model\n",
    "        model_name = f\"{start_year}.w2v\"\n",
    "        model.save(os.path.join(output_dir, model_name))\n",
    "        print(f\"Model for {start_year} saved as {model_name}\")\n",
    "\n",
    "# Train models for the specified time periods\n",
    "train_and_save_models(\n",
    "    data,\n",
    "    time_periods,\n",
    "    output_dir,\n",
    "    vector_size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context,\n",
    "    sg=1,  # Use Skip-gram\n",
    "    workers=32 \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e853359c-6e6a-4a34-a5fe-806c29fc6273",
   "metadata": {},
   "source": [
    "## Load in model to check if we can look at word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a38732d5-96bc-4f6c-8da9-b5f50ce20bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'he' and 'money': 0.36406731605529785\n",
      "Similarity between 'she' and 'money': 0.32877546548843384\n"
     ]
    }
   ],
   "source": [
    "# Load the full Word2Vec model\n",
    "model_1900 = Word2Vec.load(\"/work/NLP_exam/models/1900.w2v\") \n",
    "\n",
    "# Calculate similarity between two words\n",
    "similarity_he = model_1900.wv.similarity(\"he\", \"money\")\n",
    "print(f\"Similarity between 'he' and 'money': {similarity_he}\")\n",
    "similarity_she = model_1900.wv.similarity(\"she\", \"money\")\n",
    "print(f\"Similarity between 'she' and 'money': {similarity_she}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0663c3-1a0f-4a2a-853f-b56aba72b9c0",
   "metadata": {},
   "source": [
    "## Check the frequency of these words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43381fe4-b40d-4ecb-8c7f-8408dced6ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of 'he': 224492\n",
      "Frequency of 'she': 134317\n"
     ]
    }
   ],
   "source": [
    "def get_word_frequency(model, word):\n",
    "    if word in model.wv.key_to_index:  # Check if the word is in the model's vocabulary\n",
    "        return model.wv.get_vecattr(word, \"count\")\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Check the frequency of 'king' and 'queen'\n",
    "he_freq = get_word_frequency(model_1900, 'he')\n",
    "she_freq = get_word_frequency(model_1900, 'she')\n",
    "\n",
    "# Print the frequencies\n",
    "if he_freq is not None:\n",
    "    print(f\"Frequency of 'he': {he_freq}\")\n",
    "\n",
    "\n",
    "if she_freq is not None:\n",
    "    print(f\"Frequency of 'she': {she_freq}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
